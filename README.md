## ðŸ—ï¸ Data Architecture Diagram


flowchart LR
    A[OpenWeather API] --> B[Python Ingestion]
    B --> C[MongoDB (Raw Layer)]
    C --> D[Airflow Orchestration]
    D --> E[PostgreSQL Analytics Warehouse]
    E --> F[Dashboards & Analytics]

## SkyLogix Weather Data Pipeline
1. Project Overview
This project implements an end-to-end data engineering pipeline that ingests real-time weather data from the OpenWeather API, stores raw data in MongoDB, transforms it into an analytics-ready format, and loads it into PostgreSQL. Apache Airflow orchestrates the pipeline, enabling scheduled and reliable data processing.
The final dataset supports analytics use cases such as weather trend analysis and correlation with logistics delays.

2. Architecture & Design
OpenWeather API
      â†“
Python Ingestion Script (raw JSON)
      â†“
MongoDB (weather_raw â€“ upsert)
      â†“         (Airflow DAG)
Read from MongoDB â†’ Transform â†’ Load
      â†“
PostgreSQL (weather_readings â€“ analytics)
      â†“
Dashboards / Reports 

Key Design Choices
â€¢	MongoDB is used as a raw/staging layer to store semi-structured JSON payloads.
â€¢	PostgreSQL acts as the analytics warehouse with structured, query-optimized tables.
â€¢	Airflow schedules and orchestrates ingestion and transformation tasks.
â€¢	Python handles ingestion, transformation, and database interactions.


3. ETL Process Explaination
Extract
â€¢	Weather data is fetched from the OpenWeather API using Python.
â€¢	Data is ingested into MongoDB (weather_raw collection).
Transform
â€¢	Relevant fields (temperature, wind speed, rainfall, coordinates, conditions) are extracted.
â€¢	Data is normalized and timestamped.
â€¢	Incremental loading is supported using execution time.
Load
â€¢	Transformed data is upserted into PostgreSQL (weather_readings table).
â€¢	A unique constraint on (city, observed_at) prevents duplicates.
 ETL successfully executed and verified via PostgreSQL queries and joins.

4. Orchestration (Airflow)
â€¢	DAG: weather_pipeline
â€¢	Schedule: Every 15 minutes
â€¢	Tasks:
1.	fetch_and_upsert_raw â€“ API â†’ MongoDB
2.	transform_and_load_postgres â€“ MongoDB â†’ PostgreSQL
Task execution and logs were validated via the Airflow UI and filesystem logs.
5. Analytics Enablement
Example Queries
â€¢	Average temperature per city (last 24 hours)
â€¢	Extreme weather detection (high wind, heavy rain)
â€¢	Weather vs logistics delay analysis (JOIN with trips table)

6. Assumptions
â€¢	Weather conditions such as high wind or rainfall can be correlated with increased delivery delays, enabling proactive logistics planning.

7. Outcome
â€¢	The pipeline delivers a production-ready, analytics-enabled weather dataset that supports real-time monitoring and operational decision-making.

8. Technologies Used
â€¢	Python
â€¢	MongoDB
â€¢	PostgreSQL
â€¢	Apache Airflow
â€¢	SQL
â€¢	GitHub
â€¢	VS Code (WSL)


